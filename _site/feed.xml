<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title>honinbou</title>
	<link href="http://iguowei.net/atom.xml" rel="self" />
	<link href="http://iguowei.net/" />
	<updated>2012-11-23T19:37:12+08:00</updated>
	<id>http://iguowei.net/</id>
	<author>
		<name>honinbou</name>
		<email>weiguo85@gmail.com</email>
	</author>
	
	<entry>
		<title>amazon 2012-10-22 事故分析总结</title>
		<link href="http://iguowei.net/posts/amazon-service-event.html" />
		<updated>2012-11-22T00:00:00+08:00</updated>
		<id>http://iguowei.net/posts/amazon-service-event.html</id>
		<content type="html"><![CDATA[<p>amazon在2012年10月22在美国东海岸发生了服务故障，本文是其英文版的总结，<a href="https://aws.amazon.com/message/680342/">原文点击这里</a></p>

<h1 id="section">事故描述</h1>

<ol>
  <li>美国时间星期一10:00AM, 东海岸5个区域中的一个Zone发生少量的Elastic Block Store (EBS)服务拒绝，具体表现为无法进行IO请求,原因为agent在某个异常状态下会发生内存泄漏，而这个内存泄漏是和请求相关的；</li>
  <li>内存泄漏的EBS机器到达了某个临界值，开始发生雪崩，很快更多的机器无法处理用户请求，更多的机器无法进行IO请求；</li>
  <li>11:00AM左右，这个Zone中的大部分存储volume无法使用；</li>
  <li>11:15AM左右，为了恢复服务，团队决定减少故障转移率，减少更多机器由好的状态，变为不好的状态；</li>
  <li>11:35AM，系统开始自动恢复部分EBS 存储volume；</li>
  <li>1:40PM，60%受影响的volume被恢复，团队继续查找原因，并继续恢复余下有问题的volume。由于故障转移和恢复的量太大，导致团队很难真正定位问题；</li>
  <li>3:10PM，团队定位出真正的问题，并通过释放agent多占的内存来恢复剩下大部分未被恢复的volume；
8。4:15PM，所有受影响的存储volume均被恢复至正常状态</li>
</ol>

<h1 id="section-1">原因</h1>

<ol>
  <li>EBS 服务拒绝原因：
    <ul>
      <li>EBS 存储服务器上收集数据的agent有一个潜在的bug, 个人理解agent的作用类似于Mola的agent，个人猜测该agent既可以读数据，也可以写数据。agent设计为对时间不敏感和对数据的延时和缺失有容忍性。</li>
      <li>上周，其中一台存储机器(chunk)硬件故障并被替换，同时内部DNS(可能用做寻地址，这里估计agent查找后端的server是通过域名，而不是ip来查找)进行更新。</li>
      <li>因为没有注意到，内部DNS并没有完成更新记录成功，导致少部分存储服务器(chunk 和chunk通讯？)继续连接已经下架的机器，而不是新的好机器，因为架构设计允许数据缺失，这个问题并没有立即触发报警.</li>
      <li>但这种重试，触发了reporting agent(和刚刚的agent是一个吧？)上一个隐藏的内存泄漏的bug。而且这个泄漏是和重试次数相关的，这样就导致内存持续的被消耗。</li>
      <li>amazon有监控每个EBS server的总的内存消耗，但是这种消耗，监控系统并没有报警（或者他们的架构是reporting agent是单独的一个agent，但是和EBS server是同机器部署的，并没有监控reporting agent的内存，导致监控系统没有报警?）EBS 由于需要处理用户的数据，其使用的内存差异很大，导致无法设置关于内存使用量的报警策略。</li>
      <li>周一早上，由于内存泄漏已经非常高了，导致无法正常的处理用户的正常请求。</li>
    </ul>
  </li>
  <li>雪崩的原因
    <ul>
      <li>所有云服务的冗余设计，将请求从故障机器转移到服务OK的机器上。由于同时内存泄漏的机器太多，导致没有足够多服务状态OK的机器能够转移请求，导致更多的机器状态由OK变为不OK</li>
    </ul>
  </li>
</ol>

<h1 id="section-2">后期的防备措施</h1>

<ol>
  <li>针对这种内存泄漏，在所有EBS服务器上进行监控；</li>
  <li>下周开始，对所有的EBS服务器(包括不同地区)部署新的fix版本；</li>
  <li>修改系统监控，针对EBS服务器上每个进程进行内存消耗的监控；</li>
  <li>后续部署资源限制，阻止低优先级别的进程过多的使用机器上的资源；</li>
  <li>更新内部DNS配置，确保每次DNS的更新都是可靠的到达每台机器；更重要的是，针对这种未更新成功的情况，监控系统能够做出及时的报警；</li>
</ol>

<p>上面的这些措施，均能及早的定位引发这次事故的原因。另外，还需要评估如何修改EBS故障转移的逻辑，以避免类似这次快速的大范围的雪崩情况</p>

<h1 id="section-3">造成的影响</h1>

<h2 id="ec2和ebs-apis-的影响">对EC2和EBS APIs 的影响</h2>

<ul>
  <li>描述: 这个事故只影响了一个Zone的EBS volume服务，如果用户在其它的Zone上有足够的空间，理论上其所受的影响不大。amazon投入了大量的精力来避免service APIs受到单个Zone出事故而造成的影响。但实际上仍然有用户抱怨在这次事故中，无法进行IO操作，而此时amazon的监控系统显示EBS APIs服务正常。事后原因为Api的限流，并没有均匀的分布到所有的用户，造成部分用户被Api 限流导致无法操作资源</li>
  <li>amazon使用限流来保证内部或者外部，恶意或者非恶意的大量访问，参见[add ref]的文章，amazon的平台在设计之初，就非常关注配额和限流.对于单个应用，获取某个正确结果前的多次重试，平台是可以处理的。但是在这次事故中，所有的application一起发生故障，导致对平台产生了大量的负载，amazon有一个基础的限流政策。为了保证恢复过程中其他正常机器服务的稳定和不正常机器的恢复，团队制定了非常激进的限流措施。后面证实这个措施过于激进了。</li>
  <li>原因: 12:06PM PDT 团队执行了这个激进的限流措施，同时在监控着整体的流量和其他活动，由于监控显示正常，并没有意识到用户已经受到了显著的影响。后来意识到，少部分用户在这个限流策略下，被极高比例的限制Api调用，这些Api调用维持了大量的用户在调用他们的EC2实例和EBS资源，由于这些用户被限制的非常厉害，导致他们在事故中调用APIS服务很难成功，同时影响到他们通过AWS的管理平台来管EC2服务和EBS资源，直到 2:33PM PDT，团队才显著的减少了限流的程度</li>
  <li>后续的改进: 1. 故障处理中减少使用这种激进的处理策略，使用其它即能够恢复服务，又减少用户感知的限流策略；2. 操作dashbord上增加精细到用户级别的限流监控，而不仅仅是整体的限流监控，以减少这种情况再次发生，同时做出最准确的判断。</li>
  <li>限流是维持服务健康非常重要的工具，同时由于用户有经常更改限流强度的需求，amazon在不影响用户使用服务的前提下部署使用。从这次事故中，限流措施对许多用户造成的影响远超过团队的最初的意识。这次限流中，没有明显影响到跨多个Zone的高可用架构的程序，但影响到了许多单Zone的用户，导致他们在几个无法恢复到健康状态，甚至是正常工作的down。后面就是赔偿balala的。。。</li>
</ul>

<h2 id="rdsamazon-relational-database-service的影响">对RDS(Amazon Relational Database Service)的影响</h2>

<ul>
  <li>描述：因为RDS使用EBS来存储database和log，所以部分在受影响区域的RDS database在这次事故中也悲剧了。当然，使用非影响Zone的RDS instance是不受影响的。</li>
  <li>amazon的RDS有两种使用模式：Single Availability Zone (Single-AZ)单区域(唯一的一个single DataBase instance in one Zone)和Multi Availability Zone (Multi-AZ)多区域(两个database instance in two different Zone)，类似于mysql的主从模式，其中一个为主一个为从，正常情况下，主database hold住所有的请求，并复制到从上，在主挂掉时，从完成完整性check后，迅速的变为主。</li>
  <li>Single-AZ的database实例在受影响的Zone中被直接影响了。在事故中，如果database实例中，如果其使用的一个EBS volume有问题，则会被影响到，具体表现为IO请求异常(如上所描述)，Single-AZ的恢复，依赖于其底层使用的EBS volume的恢复。1:30PM PDT, 很多受影响的Single-AZ RDS instance被恢复，3:30PM PDT, 主要的受影响的database instance 被恢复，6:35PM PDT, 几乎所有的Single-AZ RDS instances 恢复。</li>
  <li>大部分的Multi-AZ在完成一致性check后，能够正常的切换到健康的Zone的从上，但是由于存在两个完全不同的bug(吐槽一下,bug真心多啊，哈哈)，低于10%的Mulit-AZ database instance没有自动的完成故障转移。
    <ol>
      <li>部分instance没有正确转移是因为碰到了一个不寻常的IO操作，导致转移逻辑没有正常处理，这些instance需要操作动作，并且存储的时间是11:30AM(个人估计是故障转移时需要写IO操作，而IO操作刚好是在11:30AM这个有问题的时候，导致异常，没太看明白这个bug)</li>
      <li>第二个bug的触发条件:a.master database instance 和从的连接断了一个很短的时间；b. 此时master的database instance写volume失败，两者几乎同时进行；c. 从master和从连接丢失到master database写volume失败这段时间内，master还在处理业务，而没有将其转移到其从上;在master的IO请求无法处理时，此时系统因为从的数据是过时的数据而被block住了。后续的修改：在master的Zone出现问题开始时，立马进行故障转移；由于问题涉及比较广,估计整个要到12月才能修复并完全上线。database instance在EBS volume恢复后就恢复了。在这两个bug修复后，预计Multi-AZ的问题就会被修复，这两个问题是团队后面最高的优先级,balala…</li>
    </ol>
  </li>
  <li>如果一个application是Multi-AZ，同时在一个Zone不可用后，仍然有足够的资源来进行后续操作的话，那么这个程序应该是可用的。后面又是赔偿等一些balala的…	</li>
</ul>

<h2 id="elbamazon-elastic-load-balancing的影响">对ELB(Amazon Elastic Load Balancing)的影响</h2>

<h1 id="aws目前能够完成的功能">AWS目前能够完成的功能</h1>

<h1 id="bcs已经完成的功能">之前BCS已经完成的功能</h1>

<h1 id="section-4">感想</h1>

]]></content>
	</entry>
	
	<entry>
		<title>lvs 测试</title>
		<link href="http://iguowei.net/posts/lvs-test.html" />
		<updated>2012-11-21T00:00:00+08:00</updated>
		<id>http://iguowei.net/posts/lvs-test.html</id>
		<content type="html"><![CDATA[
]]></content>
	</entry>
	
	<entry>
		<title>pyinotify使用中的陷阱</title>
		<link href="http://iguowei.net/posts/pyinotify.html" />
		<updated>2012-11-16T00:00:00+08:00</updated>
		<id>http://iguowei.net/posts/pyinotify.html</id>
		<content type="html"><![CDATA[<p>在做米聊线上监控报警工具的时候，需要将每次的监控数据进行记录，这里采用了以下流程。</p>

<p><img src="/assets/pic/2012-11-16-miliao.png" alt="米聊在线服务预警图" /></p>

<p>探测程序会定期将自己的探测结果以xml文件的形式写入到特定的目录中，监控程序会监控该目录并到新写入的数据，并插入数据库。报警程序会定期扫描数据库中的数据，结合报警策略，决定是否进行报警。</p>

<pre><code>问：为啥探测程序会是写入特定的目录中，再有监控程序写入数据库，而不是直接写数据库?

答：1. 保证探测程序的简单性，不做过多的事件。

2. 解耦合，监控程序类似于一个agent，可以支持更多不同语义的监控。

3. 监控＋报警这样更容易做成一个服务


问：为啥监控程序会将监控数据写入数据库，而不是直接决定是否报警

答：1. 报警策略可能需要结合监控的多个状态来报警。

2. 更重要的是，满足可以网页可以浏览在过去一段时间内的监控状态
</code></pre>

<p>如何获取一个指定的目录下，是否有新的数据写入？
两种方法：</p>

<ol>
  <li>启动一个线程，定期扫描该文件夹，如果某个文件不存在之前的文件hash表中，则为新增的数据。或者每次写入的文件名相同，此时，比较本次的时间和上次的时间是否相同。</li>
  <li>使用inotify，在文件夹下有自己感兴趣的event发生时，则调用回调函数，写入数据库中</li>
</ol>

<p>方法二相对于方法一，减少了定期的轮询开销，减少了前后两个状态的对比，处理方式相对更加优雅。</p>

<p>监控程序使用python编写，所以使用pyinotify来实现inotify的监控。inotify 既可以监视文件，也可以监视目录，可以监视的文件系统事件包括：</p>

<table>
	<tr>
		<td>Event Name</td><td>Is an Event</td><td> Description</td>
	</tr>
	<tr>
		<td>IN_ACCESS</td><td>YES</td><td>file was accessed</td>
	</tr>
	<tr>
		<td>IN_ATTRIB</td><td>YES</td><td>metadata changed</td>
	</tr>
	<tr>
		<td>IN_CLOSE_WRITE</td><td>YES</td><td>writtable file was closed</td>
	</tr>
	<tr>
		<td>IN_CREATE</td><td>YES</td><td>file/dir was created in watched directory</td>
	</tr>
	<tr>
		<td>IN_DELETE</td><td>YES</td><td>file/dir was deleted in watched directory</td>
	</tr>
	<tr>
		<td>IN_MODIFY</td><td>YES</td><td>file was modified</td>
	</tr>
	<tr>
		<td>IN_OPEN</td><td>YES</td><td>file was opened</td>
	</tr>	
</table>

<p>通过pyinotify来实现对文件系统的监控非常简单，如下是一个demo，只需要重写对应的回调函数既可。</p>

<pre><code>#!/usr/bin/env python
# encoding:utf-8
 
import os
from  pyinotify import  WatchManager, Notifier, \
ProcessEvent,IN_DELETE, IN_CREATE,IN_MODIFY
 
class EventHandler(ProcessEvent):
    """事件处理"""
    def process_IN_CREATE(self, event):
        print   "Create file: %s "  %   os.path.join(event.path,event.name)
 
    def process_IN_DELETE(self, event):
        print   "Delete file: %s "  %   os.path.join(event.path,event.name)
 
    def process_IN_MODIFY(self, event):
        print   "Modify file: %s "  %   os.path.join(event.path,event.name)
 
def FSMonitor(path='.'):
    wm = WatchManager() 
    mask = IN_DELETE | IN_CREATE |IN_MODIFY
    notifier = Notifier(wm, EventHandler())
    wm.add_watch(path, mask,rec=True)
    print 'now starting monitor %s'%(path)
    while True:
        try:
            notifier.process_events()
            if notifier.check_events():
                notifier.read_events()
        except KeyboardInterrupt:
            notifier.stop()
            break
 
if __name__ == "__main__":
    FSMonitor()
</code></pre>

<p>放在本项目中，则被hook的函数为IN_CREATE，因为每次探测程序都会写数据到新的文件，然后定期删除监控的数据。但是一个诡异的问题出现了：在IN_CREATE事件发生时，调用程序读取新的数据，读取函数返回错误：
	xml.parsers.expat.ExpatError: no element found: line 1, column 0</p>

<p>google 该错误信息，很多都说是open了file后忘记了关闭，但是我在这里已经做了close的动作。而且这个问题的出现，是偶尔发生的。</p>

<p>思考后，发现是读取element没有读取到，是不是和文件的content有关，没办法，只能抓日志，并查看content的日志是否合法了，有了这个思路后，下一次一发生这个文件，就找到原因了。content的长度为0，难怪会打印这个错误。</p>

<p>于是疑问来了，为啥content的内容会为0？打开文件时，明明长度是存在的。当然，sleep 1s，基本可以规避这个问题。但是明显没有完全解决这个问题。</p>

<p>后面回去的路上，仔细考虑这个问题，并将demo代码中的事件都监控起来，终于找到问题的根源了。</p>

<p>创建文件的方法为：</p>

<pre><code>f = open(filename, 'w')
...
dom.writexml(f, addindent='  ', newl='\n', encoding='utf-8')
f.close()
</code></pre>

<p>在这里，实际发生了三个事件，分别是IN_CREATE, IN_MODIFY, IN_CLOSE_WRITE, 分别对应于这三步，虽然我们平时说的是创建一个文件，但在inotify中，创建文件和我们平时指的还是有较大差别。</p>

<p>修复方法自然也出来了，将监控的事件修改为IN_CLOSE_WRITE，问题解决。sleep 只是一个不治本的方法。</p>
]]></content>
	</entry>
	
	<entry>
		<title>你好，世界</title>
		<link href="http://iguowei.net/posts/hello-world.html" />
		<updated>2012-11-07T00:00:00+08:00</updated>
		<id>http://iguowei.net/posts/hello-world.html</id>
		<content type="html"><![CDATA[<h2>你好，世界</h2>
<p>我在github上的第一篇文章，也是在github上搭建自己的blog成功的一天</p>
<p>07 Nov 2012</p>
]]></content>
	</entry>
	
</feed>