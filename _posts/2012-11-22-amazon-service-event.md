---
layout: post
title: amazon 2012-10-22 事故分析总结
category: thinking
tags: [pyinotify]
---

amazon在2012年10月22在美国东海岸发生了服务故障，本文是其英文版的总结，[原文点击这里][english-link]

# 事故描述
 
1. 美国时间星期一10:00AM, 东海岸5个区域中的一个Zone发生少量的Elastic Block Store (EBS)服务拒绝，具体表现为无法进行IO请求；
2. 内存泄漏的EBS机器到达了某个临界值，开始发生雪崩，很快更多的机器无法处理用户请求，更多的机器无法进行IO请求；
3. 11:00AM左右，这个Zone中的大部分存储volume无法使用；
4. 11:15AM左右，为了恢复服务，团队决定减少故障转移率，减少更多机器由好的状态，变为不好的状态；
5. 11:35AM，系统开始自动恢复部分EBS 存储volume；
6. 1:40PM，60%受影响的volume被恢复，团队继续查找原因，并继续恢复余下有问题的volume。由于故障转移和恢复的量太大，导致团队很难真正定位问题；
7. 3:10PM，团队定位出真正的问题，并通过释放agent多占的内存来恢复剩下大部分未被恢复的volume；
8。4:15PM，所有受影响的存储volume均被恢复至正常状态


# 原因

1. EBS 服务拒绝原因：
* EBS 存储服务器上收集数据的agent有一个潜在的bug, 个人理解agent的作用类似于Mola的agent，个人猜测该agent既可以读数据，也可以写数据。agent设计为对时间不敏感和对数据的延时和缺失有容忍性。
* 上周，其中一台存储机器(chunk)硬件故障并被替换，同时内部DNS(可能用做寻地址，这里估计agent查找后端的server是通过域名，而不是ip来查找)进行更新。
* 因为没有注意到，内部DNS并没有完成更新记录成功，导致少部分存储服务器(chunk 和chunk通讯？)继续连接已经下架的机器，而不是新的好机器，因为架构设计允许数据缺失，这个问题并没有立即触发报警.
* 但这种重试，触发了reporting agent(和刚刚的agent是一个吧？)上一个隐藏的内存泄漏的bug。而且这个泄漏是和重试次数相关的，这样就导致内存持续的被消耗。
* amazon有监控每个EBS server的总的内存消耗，但是这种消耗，监控系统并没有报警（或者他们的架构是reporting agent是单独的一个agent，但是和EBS server是同机器部署的，并没有监控reporting agent的内存，导致监控系统没有报警?）EBS 由于需要处理用户的数据，其使用的内存差异很大，导致无法设置关于内存使用量的报警策略。
* 周一早上，由于内存泄漏已经非常高了，导致无法正常的处理用户的正常请求。

2. 雪崩的原因
* 所有云服务的冗余设计，将请求从故障机器转移到服务OK的机器上。由于同时内存泄漏的机器太多，导致没有足够多服务状态OK的机器能够转移请求，导致更多的机器状态由OK变为不OK

# 后期的防备措施

1. 针对这种内存泄漏，在所有EBS服务器上进行监控；
2. 下周开始，对所有的EBS服务器(包括不同地区)部署新的fix版本；
3. 修改系统监控，针对EBS服务器上每个进程进行内存消耗的监控；
4. 后续部署资源限制，阻止低优先级别的进程过多的使用机器上的资源；
5. 更新内部DNS配置，确保每次DNS的更新都是可靠的到达每台机器；更重要的是，针对这种未更新成功的情况，监控系统能够做出及时的报警；

上面的这些措施，均能及早的定位引发这次事故的原因。另外，还需要评估如何修改EBS故障转移的逻辑，以避免类似这次快速的大范围的雪崩情况

# 造成的影响

1. 对EC2和EBS APIs 的影响

2. 对RDS(Amazon Relational Database Service)的影响

3. 对ELB(Amazon Elastic Load Balancing)的影响

# AWS目前能够完成的功能

# 之前BCS已经完成的功能

# 感想

